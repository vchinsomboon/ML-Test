{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d6607f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 324 candidates, totalling 2592 fits\n",
      "Best parameters found:  {'classifier__criterion': 'gini', 'classifier__max_depth': 20, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 2}\n",
      "Best cross-validation score: 0.81\n",
      "Test set accuracy with best model: 0.79\n",
      "Confusion Matrix:\n",
      "[[90 15]\n",
      " [23 51]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       105\n",
      "           1       0.77      0.69      0.73        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.77      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "864 fits failed out of a total of 2592.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "690 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "174 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.75140449 0.75280899 0.74859551\n",
      " 0.75702247 0.76404494 0.78792135 0.76404494 0.75421348 0.81039326\n",
      " 0.73735955 0.76685393 0.7738764  0.76123596 0.77808989 0.77668539\n",
      " 0.77247191 0.78370787 0.79073034        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.76123596 0.75983146 0.78370787 0.74438202 0.76685393 0.77247191\n",
      " 0.75561798 0.73174157 0.78792135 0.78370787 0.76404494 0.78651685\n",
      " 0.77949438 0.76544944 0.78230337 0.75421348 0.76966292 0.79634831\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.7261236  0.75983146 0.78370787\n",
      " 0.75842697 0.78089888 0.77808989 0.7738764  0.76685393 0.76966292\n",
      " 0.75280899 0.76264045 0.76264045 0.75842697 0.76685393 0.7738764\n",
      " 0.81179775 0.80337079 0.77808989        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72191011 0.75421348 0.76544944 0.76966292 0.75       0.77949438\n",
      " 0.77106742 0.75280899 0.7738764  0.76264045 0.75421348 0.76685393\n",
      " 0.77106742 0.75983146 0.78792135 0.78511236 0.76825843 0.78230337\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.73314607 0.75       0.75140449\n",
      " 0.78370787 0.75561798 0.77808989 0.7738764  0.80196629 0.77808989\n",
      " 0.73455056 0.76966292 0.78230337 0.74719101 0.74859551 0.78370787\n",
      " 0.77949438 0.7738764  0.77106742        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73735955 0.75983146 0.76544944 0.75983146 0.75       0.78651685\n",
      " 0.75       0.79353933 0.78651685 0.73455056 0.7752809  0.76685393\n",
      " 0.78792135 0.75702247 0.77668539 0.76685393 0.78089888 0.79775281\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.73455056 0.75842697 0.74016854\n",
      " 0.75561798 0.75842697 0.75983146 0.77668539 0.78370787 0.78511236\n",
      " 0.75       0.76825843 0.75983146 0.75421348 0.75561798 0.79494382\n",
      " 0.77808989 0.78230337 0.77949438        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.76404494 0.78932584 0.7738764  0.76825843 0.77949438 0.78792135\n",
      " 0.75140449 0.79073034 0.8005618  0.77808989 0.77949438 0.79353933\n",
      " 0.78792135 0.77106742 0.79073034 0.78792135 0.78230337 0.78230337\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.74719101 0.78230337 0.74297753\n",
      " 0.76685393 0.75702247 0.77668539 0.77247191 0.77668539 0.7752809\n",
      " 0.73735955 0.73595506 0.76404494 0.77668539 0.78932584 0.78651685\n",
      " 0.77949438 0.77808989 0.78089888        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.74578652 0.75280899 0.76123596 0.76685393 0.77106742 0.76825843\n",
      " 0.75561798 0.75983146 0.77949438 0.75702247 0.76825843 0.77808989\n",
      " 0.76825843 0.76544944 0.77668539 0.7738764  0.7991573  0.78932584\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.75       0.74438202 0.75983146\n",
      " 0.77949438 0.75140449 0.7991573  0.77106742 0.75561798 0.76544944\n",
      " 0.75280899 0.75421348 0.77808989 0.76404494 0.76544944 0.78932584\n",
      " 0.76544944 0.78089888 0.77247191        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.7261236  0.76685393 0.7738764  0.75       0.76264045 0.75421348\n",
      " 0.75702247 0.80196629 0.76544944 0.73735955 0.75702247 0.76123596\n",
      " 0.75702247 0.75421348 0.78230337 0.77808989 0.76544944 0.7738764 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## DECISION TREE CLASSIFIER\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Extract first letter of 'Cabin' field as a new column\n",
    "train_data['Cabin'] = train_data['Cabin'].str[0]\n",
    "\n",
    "# Assign weighted values to 'Sex'\n",
    "train_data['Sex'] = train_data['Sex'].map({'female': 1, 'male': 0})\n",
    "\n",
    "# Drop unnecessary columns from X\n",
    "X = train_data.drop(columns=['Survived', 'Name', 'Embarked', 'Ticket'])\n",
    "\n",
    "# Identify and handle empty rows\n",
    "empty_row_indices = X.index[X.isnull().all(axis=1)]  # Rows where all columns are NaN\n",
    "X.drop(empty_row_indices, inplace=True)\n",
    "\n",
    "# Calculate overall median age\n",
    "overall_median_age = X['Age'].median()\n",
    "\n",
    "# Fill missing 'Age' values based on 'SibSp' and 'Parch', with fallback to overall median\n",
    "age_median = X.groupby(['SibSp', 'Parch'])['Age'].median()\n",
    "\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        try:\n",
    "            return age_median.loc[(row['SibSp'], row['Parch'])]\n",
    "        except KeyError:\n",
    "            return overall_median_age\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "X['Age'] = X.apply(fill_age, axis=1)\n",
    "\n",
    "# Handle any remaining NaN values in 'Age' by filling with overall median\n",
    "X['Age'] = X['Age'].fillna(overall_median_age)\n",
    "\n",
    "# Encode 'Cabin' column into numerical format\n",
    "cabin_encoder = LabelEncoder()\n",
    "X['Cabin'] = cabin_encoder.fit_transform(X['Cabin'])\n",
    "\n",
    "# Preprocessing pipeline - The ColumnTransformer applies the MinMaxScaler to the specified numerical columns to standardize them where now min value is 0 and max value is 1.\n",
    "#The remainder='passthrough' parameter indicates that columns not specified in the transformers should be passed through without any changes.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), ['Age', 'Fare'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Combine preprocessor and model into a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Extract the target variable\n",
    "y = train_data['Survived']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, \n",
    "                           cv=8, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy with best model: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd370490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 24 candidates, totalling 192 fits\n",
      "Best parameters found:  {'classifier__max_depth': 10, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 200}\n",
      "Best cross-validation score: 0.82\n",
      "Test set accuracy with best model: 0.82\n",
      "Confusion Matrix:\n",
      "[[92 13]\n",
      " [20 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       105\n",
      "           1       0.81      0.73      0.77        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#RANDOM FOREST CLASSIFIER\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib  # Import joblib for model persistence\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Feature engineering\n",
    "train_data['Cabin'] = train_data['Cabin'].str[0]\n",
    "train_data['Sex'] = train_data['Sex'].map({'female': 1, 'male': 0})\n",
    "\n",
    "X = train_data.drop(columns=['Survived', 'Name', 'Embarked', 'Ticket'])\n",
    "y = train_data['Survived']\n",
    "\n",
    "overall_median_age = X['Age'].median()\n",
    "age_median = X.groupby(['SibSp', 'Parch'])['Age'].median()\n",
    "\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        try:\n",
    "            return age_median.loc[(row['SibSp'], row['Parch'])]\n",
    "        except KeyError:\n",
    "            return overall_median_age\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "X['Age'] = X.apply(fill_age, axis=1)\n",
    "X['Age'] = X['Age'].fillna(overall_median_age)\n",
    "\n",
    "cabin_encoder = LabelEncoder()\n",
    "X['Cabin'] = cabin_encoder.fit_transform(X['Cabin'])\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), ['Age', 'Fare'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Combine preprocessor and model into a pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, \n",
    "                           cv=8, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy with best model: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#joblib.dump(best_model, 'titanic-survival-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9739fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "## RANDOM FOREST CLASSIFIER\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib  # Import joblib for model persistence\n",
    "\n",
    "# Load data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Load the persisted model to make predictions\n",
    "model = joblib.load('titanic-survival-model.joblib')\n",
    "\n",
    "# Input new data points to predict their genre\n",
    "predictions = model.predict(X)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
